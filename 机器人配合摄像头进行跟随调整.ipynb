import cv2
import imutils
import math
import threading
from picamera2 import Picamera2
from IPython.display import display, Image
import ipywidgets as widgets
import mediapipe as mp
import time
from base_ctrl import BaseController  # 假设你的机器人控制库

# 创建一个 "停止" 按钮，允许用户通过点击它来停止视频流
stopButton = widgets.ToggleButton(
    value=False,
    description='Stop',
    disabled=False,
    button_style='danger',
    tooltip='点击停止视频',
    icon='square'
)

# 初始化 MediaPipe 绘图工具和手部关键点检测模型
mpDraw = mp.solutions.drawing_utils
mpHands = mp.solutions.hands
hands = mp.solutions.hands.Hands(max_num_hands=2)  # 初始化，最多检测两只手

# 初始化 OpenCV 人脸检测器（Haar Cascade）
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# 初始化机器人控制器（根据树莓派型号选择正确的串口）
base = BaseController('/dev/serial0', 115200)  # 根据硬件修改

# 记录摄像头的初始位置（原点）
initial_x, initial_y = 0, 0

# 定义显示函数，处理视频帧并进行手势检测
def view(button):
    camera = cv2.VideoCapture(0)  # 使用 OpenCV 打开相机，若有多个相机，修改为正确的设备索引
    camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    display_handle = display(None, display_id=True)  # 创建一个显示句柄来更新显示的图像

    is_adjusted = False  # 标志位，表示是否已经调整过摄像头位置

    while not stopButton.value:  # 当按钮没有被点击时
        _, frame = camera.read()
        if not _:
            break  # 如果无法捕获到帧，则退出

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)

        if len(faces) > 0:
            # 获取第一个人脸的坐标
            (x, y, w, h) = faces[0]

            # 计算人脸中心点
            face_center = (x + w // 2, y + h // 2)

            # 画出人脸框
            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)

            # 计算偏移量（相对图像中心）
            image_center = (frame.shape[1] // 2, frame.shape[0] // 2)
            offset_x = face_center[0] - image_center[0]
            offset_y = face_center[1] - image_center[1]

            # 基于偏移量计算摄像头需要调整的角度
            input_x = -offset_y / 10  # 俯仰角度（上下）
            input_y = offset_x / 10  # 水平角度（左右）

            # 限制角度范围（可根据实际需求调整）
            input_x = max(-45, min(45, input_x))
            input_y = max(-45, min(45, input_y))

            # 设置偏移量阈值，只有当偏移量超过该阈值时，才进行微调
            threshold = 20  # 可以根据实际需要调整这个阈值

            if abs(offset_x) > threshold or abs(offset_y) > threshold:
                # 如果偏移量超过阈值，才调整摄像头
                base.gimbal_ctrl(input_x, input_y, 0, 0)
                is_adjusted = True  # 标记为已经调整过摄像头
            else:
                # 如果偏移量较小，保持摄像头稳定
                if not is_adjusted:
                    base.gimbal_ctrl(initial_x, initial_y, 0, 0)
                    is_adjusted = False  # 标记为未调整过摄像头
        else:
            # 如果没有检测到人脸，摄像头回到原点
            base.gimbal_ctrl(initial_x, initial_y, 0, 0)
            is_adjusted = False  # 标记为未调整过摄像头

        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # 将图像从 BGR 转换为 RGB 格式（OpenCV 默认是 BGR）
        results = hands.process(img)

        if results.multi_hand_landmarks:
            for handLms in results.multi_hand_landmarks:  # 遍历检测到的每只手
                # 提取手势分析所需的关键点
                landmarks = handLms.landmark
                hand_type = "Left" if landmarks[0].x < 0.5 else "Right"  # 如果 x < 0.5 为左手，否则为右手

                # 计算手势，根据手指角度和位置判断
                gesture = detect_gesture(landmarks)

                # 根据手势控制机器人运动
                if hand_type == "Left":
                    if gesture == "Closed":
                        # 左手握拳 -> 向左转弯
                        base.send_command({"T": 1, "L": 0.2, "R": -0.2})
                    elif gesture == "Open":
                        # 左手张开 -> 向右转弯
                        base.send_command({"T": 1, "L": -0.2, "R": 0.2})
                elif hand_type == "Right":
                    if gesture == "Closed":
                        # 右手握拳 -> 向前走
                        base.send_command({"T": 1, "L": 0.2, "R": 0.2})
                    elif gesture == "Open":
                        # 右手张开 -> 向后退
                        base.send_command({"T": 1, "L": -0.2, "R": -0.2})

                # 在图像中绘制手的关键点和连接
                mpDraw.draw_landmarks(frame, handLms, mpHands.HAND_CONNECTIONS)

        # 编码并更新图像
        _, frame = cv2.imencode('.jpeg', frame)
        display_handle.update(Image(data=frame.tobytes()))  # 更新 Jupyter 显示

    # 按钮被点击时，停止相机并关闭
    camera.release()  # 释放相机资源
    display_handle.update(None)  # 清除显示

def detect_gesture(landmarks):
    """根据关键点位置检测手势。"""
    # 根据手指的距离判断手势是握拳（Closed）还是张开（Open）
    # 手势是通过关键点之间的距离来判断的

    thumb_tip = landmarks[mpHands.HandLandmark.THUMB_TIP]
    index_tip = landmarks[mpHands.HandLandmark.INDEX_FINGER_TIP]
    middle_tip = landmarks[mpHands.HandLandmark.MIDDLE_FINGER_TIP]
    
    # 计算拇指、食指和中指尖之间的距离
    thumb_index_distance = math.hypot(thumb_tip.x - index_tip.x, thumb_tip.y - index_tip.y)
    index_middle_distance = math.hypot(index_tip.x - middle_tip.x, index_tip.y - middle_tip.y)

    # 如果拇指和食指的距离小于阈值，且食指和中指的距离也很小，则认为是握拳（Closed）
    if thumb_index_distance < 0.05 and index_middle_distance < 0.05:
        return "Closed"  # 手势为握拳
    else:
        return "Open"  # 手势为张开

# 显示 "停止" 按钮，并启动线程执行显示函数
display(stopButton)
thread = threading.Thread(target=view, args=(stopButton,))
thread.start()
